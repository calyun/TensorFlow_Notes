{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "GRUs are simpler than LSTMs\n",
    "\n",
    "# TO create a GRU cell with a certain size:\n",
    "cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
    "#Layers of cells\n",
    "rnn_cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers)\n",
    "\n",
    "# TF can construct RNN dynamically\n",
    "#tf.nn.dynamic_rnn(cell, seq, length, initial_state)\n",
    "# adjust sequence length by padding with zeros\n",
    "\n",
    "# can ignore loss from padded label with boolean mask\n",
    "full_loss = tf.nn.softmax_cross_entropy_loss_with_logits(labels=labels, logits=logits)\n",
    "loss = tf.reduce_mean(tf.boolean_mask(full_loss, mask))\n",
    "# or adjust the length\n",
    "tf.reduce_sum(tf.reduce_max(tf.sign(seq), 2), 1)\n",
    "\n",
    "\n",
    "# Deal with vanishing gradients -- with activation units\n",
    "# Exploding gradients - use tf.clip_by_global_norm\n",
    "gradients = tf.gradients(cost, tf.trainable_variables()) # finds gradient for all trainable variables\n",
    "clipped_gradients, _ = tf.clip_by_global_norm(gradients, max_grad_norm) # clips by a max norm\n",
    "optimizer = tf.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.apply_gradients(zip(gradients, trainables))  # feeds clipped to optimizer, (why not use clipped_gradients?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Language Generation\n",
    "\n",
    "-- Word-level (limited by the vocabulary)\n",
    "-- Character-level (may produce nonsense words)\n",
    "-- Hybrid (combined word and character)\n",
    "-- Subword (most common words and syllables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Phase 1: Assemble the graph\n",
    "\n",
    "    Define placeholders for Input and Output\n",
    "    Define Weights\n",
    "    Define Inference Model\n",
    "    Define Loss Function\n",
    "    Define Optimizer (e.g. GradientDescentOptimizer, AdamOptimizer)\n",
    "\n",
    "Phase 2: Train the model\n",
    "\n",
    "    Initialize all model variables (e.g. tf.global_variables_initializer())\n",
    "    Feed in the training data (e.g. batching, randomization)\n",
    "    Execute inference model on the training inputs with the current model parameters\n",
    "    Compute the cost\n",
    "    Adjust model parameters accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CS20SI Github's Character-Level Generative Language Model\n",
    "\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "DATA_PATH = 'data/arxiv_abstracts.txt'\n",
    "HIDDEN_SIZE = 200\n",
    "\n",
    "# training params\n",
    "BATCH_SIZE = 64\n",
    "NUM_STEPS = 50\n",
    "SKIP_STEP = 40\n",
    "# lower temperature -> more conservative, higher temperature -> more risk/diversity\n",
    "TEMPERATURE = 0.7\n",
    "LR = 0.003\n",
    "LEN_GEN = 300\n",
    "\n",
    "def vocab_encode(text, vocab):\n",
    "    # assigns numerical value depending on position in vocab dict\n",
    "    return [vocab.index(x) + 1 for x in text if x in vocab]\n",
    "\n",
    "def vocab_decode(array, vocab):\n",
    "    # given array of numbers, decode into words\n",
    "    return ''.join([vocab[x - 1] for x in array])\n",
    "\n",
    "def read_data(filename, vocab, window=NUM_STEPS, overlap=NUM_STEPS/2):\n",
    "    # encode words in a file\n",
    "    for text in open(filename):\n",
    "        text = vocab_encode(text, vocab)\n",
    "        for start in range(0, len(text) - window, overlap):\n",
    "        # generates word chunks of length window, followed by [0]'s if padding needed\n",
    "            chunk = text[start: start + window]\n",
    "            chunk += [0] * (window - len(chunk))\n",
    "            yield chunk\n",
    "        # yield creates a Generator object;\n",
    "        # only when that Generator is run are those operators in the for loop performed\n",
    "        \n",
    "def read_batch(stream, batch_size=BATCH_SIZE):\n",
    "    batch = []\n",
    "    # creates a batch composed of elements from stream\n",
    "    for element in stream:\n",
    "        batch.append(element)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    yield batch\n",
    "    \n",
    "def create_rnn(seq, hidden_size=HIDDEN_SIZE):\n",
    "    # makes an RNN with GRU cells\n",
    "    cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
    "    default_input = cell.zero_state(tf.shape(seq)[0], tf.float32)\n",
    "    # state to feed into RNN\n",
    "    in_state = tf.placeholder_with_default(default_input, [None, hidden_size])    \n",
    "    \n",
    "    # finds total sum --> considers padding of seq\n",
    "    # reduce_max --> gives max length\n",
    "    # reduce_sum --> total of the lengths\n",
    "    length = tf.reduce_sum(tf.reduce_max(tf.sign(seq), 2), 1)\n",
    "    \n",
    "    # creates RNN based on cell, feeds in the sequence as input, with initial_state=in_state\n",
    "    output, out_state = tf.nn.dynamic_rnn(cell, seq, length, in_state)\n",
    "    \n",
    "    return output, in_state, out_state\n",
    "    \n",
    "\n",
    "def create_model(seq, temp, vocab, hidden=HIDDEN_SIZE):\n",
    "    # seq is the ARRAY of INDICES (representing the characters)\n",
    "    # creates one_hot tensor with 1 at seq, with HxW - [#indices x len(vocab)]\n",
    "    # -- array depicting character-level\n",
    "    seq = tf.one_hot(seq, len(vocab))\n",
    "    # load the above one_hot tensor into RNN with #units = hidden\n",
    "    output, in_state, out_state = create_rnn(seq, hidden)\n",
    "    \n",
    "    # adds a fully connected layer -- feed in \"outputs\", w/ #outputunits = len(vocab)\n",
    "    # returns the resulting tensor (predictions from going through the RNN)\n",
    "    logits = tf.contrib.layers.fully_connected(output, len(vocab), None)\n",
    "    # None to skip default ReLU and keep linear\n",
    "    \n",
    "    # logits are the predict\n",
    "    loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits[:, :-1], labels=seq[:, 1:]))\n",
    "    sample = tf.multinomial(tf.exp(logits[:,-1] / temp), 1)[:, 0]\n",
    "    # gives the softmax cross-entropy loss, a single character, and RNN in_state and out_state tensors\n",
    "    return loss, sample, in_state, out_state\n",
    "    \n",
    "# Step 1: Define inference model\n",
    "#(weights, etc. administered by TF RNN implementation)\n",
    "def online_inference(sess, vocab, seq, sample, temp, in_state, out_state, seed=\"T\"):\n",
    "    \"\"\" Dynamically generates the seq based on previous characters \"\"\"\n",
    "    sentence = seed # begins with first letter\n",
    "    state = None\n",
    "    for _ in range(LEN_GEN):\n",
    "        batch = [vocab_encode(sentence[-1], vocab)]\n",
    "        feed_dict = {seq: batch, temp: TEMPERATURE}\n",
    "        if state is not None:\n",
    "            # feeds in the state in feed_dict\n",
    "            feed_dict.update({in_state: state})\n",
    "        # returns index and new state from running the out_state op\n",
    "        # out_state from the dynamic RNN in createRNN -> createModel -> main -> online_inference ... phew!\n",
    "        # Recall RNN cell has in_state, out_state, and output\n",
    "        index, state = sess.run([sample, out_state], feed_dict)\n",
    "        sentence += vocab_decode(index, vocab)\n",
    "    print(sentence)\n",
    "    \n",
    "def training(vocab, seq, loss, optimizer, global_step, temp, sample, in_state, out_state):\n",
    "    saver = tf.train.Saver()\n",
    "    start = time.time()\n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter('graphs/gist', sess.graph)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/arxiv/checkpoint'))\n",
    "        # if checkpoint exists and path is given, restore sess from path\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            \n",
    "        iteration = global_step.eval() # gets current iteration\n",
    "        for batch in read_batch(read_data(DATA_PATH, vocab)):\n",
    "            # this is the training part -> uses optimizer against loss in the model defined in above functions\n",
    "            batch_loss, _ = sess.run([loss, optimizer], {seq: batch})\n",
    "            # status update\n",
    "            if (iteration + 1) % SKIP_STEP == 0:\n",
    "                print('Iter {}. \\n Loss {}. Time {}'.format(iteration, batch_loss, time.time() - start))\n",
    "                online_inference(sess, vocab, seq, sample, temp, in_state, out_state)\n",
    "                start = time.time()\n",
    "                saver.save(sess, 'checkpoints/arxiv/char-rnn', iteration)\n",
    "            iteration += 1\n",
    "    \n",
    "def main():\n",
    "    vocab = (\n",
    "            \" $%'()+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "            \"\\\\^_abcdefghijklmnopqrstuvwxyz{|}\")\n",
    "    # Input Placeholder\n",
    "    seq = tf.placeholder(tf.int32, [None, None])\n",
    "    # Temp\n",
    "    temp = tf.placeholder(tf.float32)\n",
    "    loss, sample, in_state, out_state = create_model(seq, temp, vocab)\n",
    "    global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "    optimizer = tf.train.AdamOptimizer(LR).minimize(loss, global_step=global_step)\n",
    "    # after initializing optimizer, inputs, etc.\n",
    "    training(vocab, seq, loss, optimizer, global_step, temp, sample, in_state, out_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 39. \n",
      " Loss 9315.3046875. Time 6.67547893524\n",
      "Thi e  e  e  e  e  e  e   e  e   e   e     e             e                       e             e                                                                                                        e                                                                                                    \n",
      "Iter 79. \n",
      " Loss 7994.96337891. Time 6.89592599869\n",
      "The the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the t\n",
      "Iter 119. \n",
      " Loss 7350.33837891. Time 6.50276684761\n",
      "The the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere the sere \n",
      "Iter 159. \n",
      " Loss 6878.93164062. Time 6.30432987213\n",
      "The the the the the the the the the the the the the the the the the the the the the the the the the the the the the serent of the the the the the serent of the serent of the serent of the serent of the serent of the serent of the the the the the the the the the the the the the the the the the the the\n",
      "Iter 199. \n",
      " Loss 6335.0625. Time 7.54324698448\n",
      "The senting and the convex of the convex of the convex of the convex of the convex of the convex of the convex of the convex of the convex of the convex of the convex of the convex of the convex of the convex of the convex of the convex of the convex of the convex of the convex of the convex of the c\n",
      "Iter 239. \n",
      " Loss 6141.41113281. Time 6.63791298866\n",
      "The and the and and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and and the and the and the and the and the and and the and t\n",
      "Iter 279. \n",
      " Loss 5892.30664062. Time 6.14731001854\n",
      "The arg intration the arger are the arger are the arger are the arger are the arger ard the mathing are the arger ard the mathing are the arger ard the mathing are the arger are the arger are the arger are the arger are the arger are the arger ard the mathing are the arger ard the mathing are the arg\n",
      "Iter 319. \n",
      " Loss 6075.89794922. Time 5.9538898468\n",
      "The arger and the convex on a propose a deep networks in a propose a deep networks in a propose a deep networks in a propose a deep networks in a propose a deep networks in a propose a deep networks in a propose a deep networks in a propose a deep networks in a propose a deep networks in a propose a \n",
      "Iter 359. \n",
      " Loss 5341.34667969. Time 5.76754593849\n",
      "The arger and the proposed the stach a deep network (DNN) and the propose a deep learning retrod stach network (DNN) and the propose a deep learning retrod stach network (DNN) and the propose a deep learning retrod stach network (DNN) and the propose a deep learning retrod stach network (DNN) and the\n",
      "Iter 399. \n",
      " Loss 5225.81152344. Time 5.95883178711\n",
      "The and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and a\n",
      "Iter 439. \n",
      " Loss 5034.546875. Time 5.73808503151\n",
      "The and the propost of the provide the such as the detions of the and the propost of the provide the such as the detions of the and the propost of the provide the such as the detions of the and the propored that the convexting the convexting the convexting the convexting the convexting the convexting\n",
      "Iter 479. \n",
      " Loss 4340.9921875. Time 5.81140089035\n",
      "The a simple the such as the recurrent neural networks (DNNs) and information in the representations of the stade computation of the stade computation of the stade computation of the stade computation of the stade computation of the stade computation of the stade computation of the stade computation \n",
      "Iter 519. \n",
      " Loss 4523.13964844. Time 5.97383499146\n",
      "The convex optimization the convex optimization the convex optimization the convex optimization the convex optimization the convex optimization the convex optimization the convex optimization the convex optimization the convex optimization the convex optimization the convex optimization the convex op\n",
      "Iter 559. \n",
      " Loss 5120.1875. Time 5.83000016212\n",
      "The and the prediction of the experiments that in a simple models and stace of the present a conserting the emportant of the experiments that in a semper and stace-of-the-art machine learning and stace-of-the-art machine learning and stace-of-the-art machine learning and stace-of-the-art machine lear\n",
      "Iter 599. \n",
      " Loss 4424.69433594. Time 5.76489496231\n",
      "The and the training of the network (DNNs) and the training of the network (DNNs) and the training of the network (DNNs) and the training of the network (DNNs) and the training of the network (DNNs) and the training of the network (DNNs) and the training of the network (DNNs) and the training of the \n",
      "Iter 639. \n",
      " Loss 3823.40673828. Time 5.88981699944\n",
      "The convex optimization processing the convex optimization processing the convex optimization processing the convex optimization processing the convex optimization processing the convex optimization processing the convex optimization processing the convex optimization processing the convex optimizati\n",
      "Iter 679. \n",
      " Loss 4154.96435547. Time 6.20491695404\n",
      "The and the art in a to a state-of-the-art recurrent neural network architecture and the state-of-the-art the such as a to a state-of-the-art recurrent neural network architecture and the state-of-the-art the such as a to a state-of-the-art recurrent neural network architecture and the state-of-the-a\n",
      "Iter 719. \n",
      " Loss 3528.60058594. Time 6.45409107208\n",
      "The proposed by the recently propestication of the model in the computational to explore a simele standard model in the computational to explore a simele standard model in the computational to explore a simele standard model in the computational to explore a simele standard model in the computational\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-58ca95c5b364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-f36b1848a4ac>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;31m# after initializing optimizer, inputs, etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-f36b1848a4ac>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(vocab, seq, loss, optimizer, global_step, temp, sample, in_state, out_state)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mread_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# this is the training part -> uses optimizer against loss in the model defined in above functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0;31m# status update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mSKIP_STEP\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 896\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    897\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1279\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1280\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1283\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1262\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1263\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
