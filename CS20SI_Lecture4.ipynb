{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4\n",
    "\n",
    "** Notes by Haoyu Yun **\n",
    "\n",
    "### In NLP - Counting vs Predicting\n",
    " Counting - store word associations in matrix - very expensive to store, even with SVD\n",
    "\n",
    "### PREDICTING\n",
    " Continuous bag of words - skip a word, predict it based on context\n",
    "\n",
    " Skip-gram - given a word, predict its context\n",
    " Use one-hot vector for each word; learn the weight matrix - one row for each word\n",
    " Training samples -- pairs of words in context\n",
    " e.g. \"The quick brown fox\" w/ window size 2 gives (the, quick) (the, brown) (quick, brown) (quick, fox) (brown, fox)\n",
    "\n",
    " Weight essentially like Lookup table -> returns probability vector for the given word\n",
    " --> Aim: similar words will have similar contexts --> will have similar word vectors :)\n",
    "\n",
    " Softmax denominator is expensive (exponential of dot product of every word's dot product...)\n",
    "\n",
    " Sampling (w/ e.g. NCE) gives good approximation\n",
    "\n",
    "### NCE - Noise contrastive estimation\n",
    " Goal is learn probability distribution across whole vocabulary, which is modelable with function\n",
    " --> replace denom with Z, a function of cost\n",
    "\n",
    " First, take a central word (w) to use\n",
    " obtain words (d) from a true distribution (context words) and noise distribution (random words)\n",
    "\n",
    "\n",
    "Produce probability of (d, w) with true and noise words\n",
    "\n",
    "\n",
    "Empirical distribution unknown - replace with model dtribution\n",
    "\n",
    "Z may actually be estimated as a parameter - no need to calculate it out (or even fix Z = 1 to self-normalize)\n",
    "\n",
    "We don't need to loop over the entire vocabulary at all -- we can replace with an approximation\n",
    "Empirical distribution unknown - replace with model dtribution\n",
    "\n",
    "​\n",
    "\n",
    "Z may actually be estimated as a parameter - no need to calculate it out (or even fix Z = 1 to self-normalize)\n",
    "\n",
    "​\n",
    "\n",
    "We don't need to loop over the entire vocabulary at all -- we can replace with an approximation\n",
    "\n",
    "​\n",
    "\n",
    "Key -- we only need to update gradients for this subsample of words (true + noise)\n",
    "\n",
    "Cross-entropy loss\n",
    "\n",
    "Link\n",
    "\n",
    "Measures loss in cases of probabilistic classification\n",
    "Key -- we only need to update gradients for this subsample of words (true + noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy loss\n",
    "\n",
    "[Link](https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/ \"Intro to Cross Entropy Loss\")\n",
    "\n",
    "Measures loss in cases of *probabilistic classification*\n",
    "\n",
    "Let's say you have binary identifiers for products in a store and you want to use as few bits as possible to communicate them. If you know the product distribution, you would allocate the fewest bits to the most common products.\n",
    "Similarly, entropy is the optimal loss.\n",
    "\n",
    "Cross entropy is loss with a different distribution (so it would be more costly than entropy).\n",
    "\n",
    "Relative entropy (Kullback-Leibler divergence) - how much P(i) tells us about Q(i) for each i, for distributions P and Q\n",
    "* expected value of logarithmic difference between P and Q\n",
    "\n",
    "Our objective is to tune the parameters to discriminate as well as possible -- maximize the predictive power.\n",
    "\n",
    "We want to maximize the **likelihood**, which is the product of (probabilities for each true label).\n",
    "To maximize this, we minimize the negative log.\n",
    "\n",
    "$$ L(\\{y^{(n)}\\}, \\{\\hat{y}^{(n)}\\}) = \\prod_n L(y^{(n)}, \\hat{y}^{(n)}) $$\n",
    "\n",
    "$$ \\log L(y^{(n)}, \\hat{y}^{(n)}) = \\sum_i y^{(n)}_i \\log \\hat{y}^{(n)}_i $$\n",
    "\n",
    "$$ -\\log L(\\{y^{(n)}\\}, \\{\\hat{y}^{(n)}\\}) = \\sum_n \\big[-\\sum_i y_i \\log \\hat{y}^{(n)}_i\\big] = \\sum_n H(y^{(n)}, \\hat{y}^{(n)})$$\n",
    " \n",
    "To maximize the likelihood, we minimize the cross-entropy loss!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec in TensorFlow\n",
    "\n",
    "*From the Lecture Notes:*\n",
    "\n",
    "TensorFlow model structure\n",
    "\n",
    "**Phase 1: Assemble the graph**\n",
    "1. Define placeholders for Input and Output\n",
    "2. Define Weights\n",
    "3. Define Inference Model\n",
    "4. Define Loss Function\n",
    "5. Define Optimizer\n",
    "(e.g. GradientDescentOptimizer, AdamOptimizer)\n",
    "\n",
    "**Phase 2: Train the model**\n",
    "1. Initialize all model variables (e.g. tf.global_variables_initializer())\n",
    "2. Feed in the training data (e.g. batching, randomization)\n",
    "3. Execute inference model on the training inputs with the current model parameters\n",
    "4. Compute the cost\n",
    "5. Adjust model parameters accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "We can use name_scope to group together related nodes\n",
    "\n",
    "with tf.name_scope(name_of_that_scope):\n",
    "    op_1 = ...\n",
    "    op_2 = ...\n",
    "    \n",
    "    \n",
    "We can use t-SNE (dimensionality reduction) for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name histogram loss is illegal; using histogram_loss instead.\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'Documents/0 TensorFlow Tutorials/data/text8.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-67307a8db27b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-67307a8db27b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSkipGramModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBED_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_SAMPLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mbatch_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSKIP_WINDOW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_TRAIN_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWEIGHTS_FLD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hunteryun/Documents/0 TensorFlow Tutorials/process_data.py\u001b[0m in \u001b[0;36mprocess_data\u001b[0;34m(vocab_size, batch_size, skip_window)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_window\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFILE_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEXPECTED_BYTES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hunteryun/Documents/0 TensorFlow Tutorials/process_data.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(file_name, expected_bytes)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dataset ready\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDOWNLOAD_URL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mfile_stat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile_stat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_size\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mexpected_bytes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib.pyc\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data, context)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_urlopener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0murlcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_urlopener\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self, url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m                 \u001b[0mtfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                 \u001b[0;32mimport\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'Documents/0 TensorFlow Tutorials/data/text8.zip'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
