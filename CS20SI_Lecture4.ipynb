{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4\n",
    "\n",
    "** Notes by Haoyu Yun **\n",
    "\n",
    "### In NLP - Counting vs Predicting\n",
    " Counting - store word associations in matrix - very expensive to store, even with SVD\n",
    "\n",
    "### PREDICTING\n",
    " Continuous bag of words - skip a word, predict it based on context\n",
    "\n",
    " Skip-gram - given a word, predict its context\n",
    " Use one-hot vector for each word; learn the weight matrix - one row for each word\n",
    " Training samples -- pairs of words in context\n",
    " e.g. \"The quick brown fox\" w/ window size 2 gives (the, quick) (the, brown) (quick, brown) (quick, fox) (brown, fox)\n",
    "\n",
    " Weight essentially like Lookup table -> returns probability vector for the given word\n",
    " --> Aim: similar words will have similar contexts --> will have similar word vectors :)\n",
    "\n",
    " Softmax denominator is expensive (exponential of dot product of every word's dot product...)\n",
    "\n",
    " Sampling (w/ e.g. NCE) gives good approximation\n",
    "\n",
    "### NCE - Noise contrastive estimation\n",
    " Goal is learn probability distribution across whole vocabulary, which is modelable with function\n",
    " --> replace denom with Z, a function of cost\n",
    "\n",
    " First, take a central word (w) to use\n",
    " obtain words (d) from a true distribution (context words) and noise distribution (random words)\n",
    "\n",
    "\n",
    "Produce probability of (d, w) with true and noise words\n",
    "\n",
    "\n",
    "Empirical distribution unknown - replace with model dtribution\n",
    "\n",
    "Z may actually be estimated as a parameter - no need to calculate it out (or even fix Z = 1 to self-normalize)\n",
    "\n",
    "We don't need to loop over the entire vocabulary at all -- we can replace with an approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy loss\n",
    "\n",
    "[Link]: https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/ \"Intro to Cross Entropy Loss\"\n",
    "\n",
    "Measures loss in cases of *probabilistic classification*\n",
    "\n",
    "Let's say you have binary identifiers for products in a store and you want to use as few bits as possible to communicate them. If you know the product distribution, you would allocate the fewest bits to the most common products.\n",
    "Similarly, entropy is the optimal loss.\n",
    "\n",
    "Cross entropy is loss with a different distribution (so it would be more costly than entropy).\n",
    "\n",
    "Relative entropy (Kullback-Leibler divergence) - how much P(i) tells us about Q(i) for each i, for distributions P and Q\n",
    "* expectedv value of logarithmic difference between P and Q\n",
    "\n",
    "Our objective is to tune the parameters to discriminate as well as possible -- maximize the predictive power.\n",
    "\n",
    "We want to maximize the **likelihood**, which is the product of (probabilities for each true label).\n",
    "To maximize this, we minimize the negative log.\n",
    "\n",
    "$$ L(\\{y^{(n)}\\}, \\{\\hat{y}^{(n)}\\}) = \\prod_n L(y^{(n)}, \\hat{y}^{(n)}) $$\n",
    "\n",
    "$$ \\log L(y^{(n)}, \\hat{y}^{(n)}) = \\sum_i y^{(n)}_i \\log \\hat{y}^{(n)}_i $$\n",
    "\n",
    "$$ -\\log L(\\{y^{(n)}\\}, \\{\\hat{y}^{(n)}\\}) = \\sum_n \\big[-\\sum_i y_i \\log \\hat{y}^{(n)}_i\\big] = \\sum_n H(y^{(n)}, \\hat{y}^{(n)})$$\n",
    " \n",
    "To maximize the likelihood, we minimize the cross-entropy loss!\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
