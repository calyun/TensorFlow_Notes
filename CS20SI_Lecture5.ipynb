{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.gradients(y, [xs]) finds gradient y w.r.t xs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Object-oriented program\n",
    "\n",
    "Model class\n",
    "\n",
    "__init__: initialize, with config and so on\n",
    "loss, run_epoch, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.train.Saver saves the variables of graph (and entire graph) in binary files\n",
    "\n",
    "# All the variables are in the session\n",
    "tf.train.Saver.save(sess, save_path, global_step=None)\n",
    "saver.restore(sess, 'checkpoints/checkpoint_name')\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with t.Session() as sess:\n",
    "    for step in range(..)\n",
    "    ru optimizer\n",
    "    if(step+1)%1000==0:\n",
    "        saver.save(sess, 'checkpoint_directory/model_name',\n",
    "                  global_step=model.global_step)\n",
    "        \n",
    "\n",
    "# Global step counter\n",
    "self.global_step = tf.Variable(0, dtype=tf.int32, trainable=false, name='global_step')\n",
    "\n",
    "# Create summaries\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            tf.summary.scalar(\"loss\", self.loss)\n",
    "            tf.summary.scalar(\"accuracy\", self.accuracy)\n",
    "            tf.summary.histogram(\"histogram loss\", self.loss)\n",
    "            tf.summary.image()\n",
    "            # merge all summaries in one op to simplify -- only need to run one\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "            \n",
    "# Run them\n",
    "sess.run(summary_op)\n",
    "\n",
    "# Write summaries to file\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "writer.add_summary(summary, global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "from process_data import process_data\n",
    "\n",
    "\n",
    "VOCAB_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "EMBED_SIZE = 64\n",
    "SKIP_WINDOW = 2\n",
    "NUM_SAMPLED = 64 # num of noise samples (per true sample)\n",
    "LEARNING_RATE = 1.0\n",
    "NUM_TRAIN_STEPS = 100000\n",
    "#WEIGHTS_FLD = 'processed/'\n",
    "WEIGHTS_FLD = None\n",
    "REPORT_STEP = 2000\n",
    "\n",
    "# ESTABLISH MODEL\n",
    "class SkipGramModel:\n",
    "    \"\"\" builds the graph for word2vec model \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, batch_size, num_sampled, learning_rate):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_sampled = num_sampled\n",
    "        self.learning_rate = learning_rate\n",
    "        # number of total steps\n",
    "        self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "    \n",
    "    def _create_placeholders(self):\n",
    "        # Input/Output\n",
    "        with tf.name_scope('data'):\n",
    "            self.center_words = tf.placeholder(tf.int32, shape=[self.batch_size], name='center_words')\n",
    "            self.target_words = tf.placeholder(tf.int32, shape=[self.batch_size, 1], name='target_words')\n",
    "    \n",
    "    def _create_embedding(self):\n",
    "        # Weight --> here, the Embedding Matrix\n",
    "        with tf.name_scope('embed'):\n",
    "            self.embed_matrix = tf.Variable(tf.random_uniform([self.vocab_size, self.embed_size], -1.0, 1.0),\n",
    "                                            name='embed_matrix')\n",
    "    \n",
    "    def _create_loss(self):\n",
    "        with tf.name_scope('loss'):\n",
    "            # Inference\n",
    "            # tf.nn.embedding_lookup(params, ids, partition_strategy='mod', name=None, validate_indices=True, max_norm=None)\n",
    "            # Look up the corresponding rows of center_words in the embedding matrix\n",
    "            embed = tf.nn.embedding_lookup(self.embed_matrix, self.center_words, name='embed')\n",
    "            # Loss function\n",
    "            # tf.nn.nce_loss(weights, biases, labels, inputs, num_sampled, num_classes, num_true=1, sampled_values=None, remove_accidental_hits=False, partition_strategy='mod', name='nce_loss')\n",
    "            nce_weight = tf.Variable(tf.truncated_normal([self.vocab_size, self.embed_size],\n",
    "                                                         stddev=1.0 / self.embed_size ** 0.5), name='nce_weight')\n",
    "            nce_bias = tf.Variable(tf.zeros([self.vocab_size]), name='nce_bias')\n",
    "\n",
    "            self.loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight,\n",
    "                                                biases=nce_bias,\n",
    "                                                labels=self.target_words,\n",
    "                                                inputs=embed,\n",
    "                                                num_sampled=self.num_sampled,\n",
    "                                                num_classes=self.vocab_size), name='loss')\n",
    "    \n",
    "    def _create_optimizer(self):\n",
    "        # Optimizer\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "    def _create_summaries(self):\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            tf.summary.scalar(\"loss\", self.loss)\n",
    "            tf.summary.histogram(\"histogram loss\", self.loss)\n",
    "            # merges summaries together for easier viewing\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "            \n",
    "    def build_graph(self):\n",
    "        self._create_placeholders()\n",
    "        self._create_embedding()\n",
    "        self._create_loss()\n",
    "        self._create_optimizer()\n",
    "        self._create_summaries()\n",
    "\n",
    "# TRAIN MODEL\n",
    "def train_model(model, batch_gen, num_train_steps, weights_fld):\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    initial_step = 0\n",
    "\n",
    "    sess = tf.Session()\n",
    "    # initializer must always be hrere\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # saver.save(sess, 'my-model') which calls export_meta_graph implicitly\n",
    "    # Saves a checkpoint!\n",
    "    ckpt = \n",
    "    saver.restore(tf.train.latest_checkpoint\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    writer = tf.summary.FileWriter('improved_graph/lr' + str(LEARNING_RATE), sess.graph)\n",
    "    initial_step = model.global_step.eval()\n",
    "    for index in range(initial_step, initial_step + num_train_steps):\n",
    "        # next batch\n",
    "        centers, targets = batch_gen.next()\n",
    "        feed_dict = {model.center_words: centers, target_words: targets}\n",
    "        loss_batch, _, summary = sess.run([model.loss, model.optimizer, model.summary_op], feed_dict=feed_dict)\n",
    "        writer.add_summary(summary, global_step=index)\n",
    "        total_loss += loss_batch\n",
    "        if (index + 1) % REPORT_STEP == 0:\n",
    "            print('Average loss at step {}: {:S.1f}'.format(index + 1, average_loss / (index + 1)))\n",
    "            total_loss = 0.0\n",
    "            saver.save(sess, 'checkpoints/skip-gram', index)\n",
    "    \n",
    "    # visualize the embeddings\n",
    "    final_embed_matrix = sess.run(model.embed_matrix)\n",
    "    embedding_var = tf.Variable(final_embed_matrix[:1000], name='embedding')\n",
    "    sess.run(embedding_var.initializer)\n",
    "    \n",
    "    # establish config file\n",
    "    config = projector.ProjectorConfig()\n",
    "    summary_writer = tf.summary.FileWriter('processed')\n",
    "    \n",
    "    # add embeddings to config\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = embedding_var.name\n",
    "    \n",
    "    # links to metadata\n",
    "    embedding.metadata_path = 'processed/vocab_1000.tsv'\n",
    "    \n",
    "    # saves config\n",
    "    projector.visualize_embeddings(summary_writer, config)\n",
    "    saver_embed = tf.train.Saver([embedding_var])\n",
    "    saver_embed.save(sess, 'processed/model3.ckpt', 1)\n",
    "            \n",
    "            \n",
    "def main():\n",
    "    model = SkipGramModel(VOCAB_SIZE, EMBED_SIZE, BATCH_SIZE, NUM_SAMPLED, LEARNING_RATE)\n",
    "    model.build_graph()\n",
    "    batch_gen = process_data(VOCAB_SIZE, BATCH_SIZE, SKIP_WINDOW)\n",
    "    train_model(model, batch_gen, NUM_TRAIN_STEPS, WEIGHTS_FLD)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
