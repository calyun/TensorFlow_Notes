{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" -----LECTURE 3----- \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 1624.41285565\n",
      "Epoch 1: 1598.92220243\n",
      "Epoch 2: 1574.484648\n",
      "Epoch 3: 1550.39099114\n",
      "Epoch 4: 1526.66545256\n",
      "Epoch 5: 1503.31898407\n",
      "Epoch 6: 1480.35511446\n",
      "Epoch 7: 1457.77354129\n",
      "Epoch 8: 1435.57225727\n",
      "Epoch 9: 1413.74819006\n",
      "Epoch 10: 1392.297495\n",
      "Epoch 11: 1371.21619373\n",
      "Epoch 12: 1350.49991696\n",
      "Epoch 13: 1330.14417187\n",
      "Epoch 14: 1310.14440843\n",
      "Epoch 15: 1290.49601773\n",
      "Epoch 16: 1271.19438766\n",
      "Epoch 17: 1252.23509139\n",
      "Epoch 18: 1233.61354973\n",
      "Epoch 19: 1215.32507579\n",
      "Epoch 20: 1197.36536934\n",
      "Epoch 21: 1179.72974078\n",
      "Epoch 22: 1162.4138803\n",
      "Epoch 23: 1145.41353371\n",
      "Epoch 24: 1128.72435833\n",
      "Epoch 25: 1112.34189542\n",
      "Epoch 26: 1096.2621054\n",
      "Epoch 27: 1080.4808465\n",
      "Epoch 28: 1064.99407741\n",
      "Epoch 29: 1049.79743331\n",
      "Epoch 30: 1034.88715008\n",
      "Epoch 31: 1020.25922421\n",
      "Epoch 32: 1005.90984444\n",
      "Epoch 33: 991.834992817\n",
      "Epoch 34: 978.030820756\n",
      "Epoch 35: 964.493518557\n",
      "Epoch 36: 951.219538235\n",
      "Epoch 37: 938.205065863\n",
      "Epoch 38: 925.446617399\n",
      "Epoch 39: 912.940480096\n",
      "Epoch 40: 900.682865438\n",
      "Epoch 41: 888.670434316\n",
      "Epoch 42: 876.899655206\n",
      "Epoch 43: 865.367074966\n",
      "Epoch 44: 854.069318221\n",
      "Epoch 45: 843.002973258\n",
      "Epoch 46: 832.164579892\n",
      "Epoch 47: 821.551200461\n",
      "Epoch 48: 811.159117718\n",
      "Epoch 49: 800.985318614\n",
      "Epoch 50: 791.026472049\n",
      "Epoch 51: 781.279450286\n",
      "Epoch 52: 771.741070305\n",
      "Epoch 53: 762.408123845\n",
      "Epoch 54: 753.277687765\n",
      "Epoch 55: 744.346454251\n",
      "Epoch 56: 735.611550887\n",
      "Epoch 57: 727.069767685\n",
      "Epoch 58: 718.718427244\n",
      "Epoch 59: 710.554409684\n",
      "Epoch 60: 702.574557771\n",
      "Epoch 61: 694.776019678\n",
      "Epoch 62: 687.156029922\n",
      "Epoch 63: 679.711579116\n",
      "Epoch 64: 672.439932398\n",
      "Epoch 65: 665.338282332\n",
      "Epoch 66: 658.403499969\n",
      "Epoch 67: 651.633402429\n",
      "Epoch 68: 645.024590638\n",
      "Epoch 69: 638.574363416\n",
      "Epoch 70: 632.280272471\n",
      "Epoch 71: 626.13959806\n",
      "Epoch 72: 620.149557636\n",
      "Epoch 73: 614.307590286\n",
      "Epoch 74: 608.610831545\n",
      "Epoch 75: 603.056577858\n",
      "Epoch 76: 597.642469747\n",
      "Epoch 77: 592.366090343\n",
      "Epoch 78: 587.224564832\n",
      "Epoch 79: 582.215406523\n",
      "Epoch 80: 577.336060942\n",
      "Epoch 81: 572.584170381\n",
      "Epoch 82: 567.95719647\n",
      "Epoch 83: 563.452607991\n",
      "Epoch 84: 559.0680457\n",
      "Epoch 85: 554.801070734\n",
      "Epoch 86: 550.64921615\n",
      "Epoch 87: 546.610302494\n",
      "Epoch 88: 542.681826452\n",
      "Epoch 89: 538.861624504\n",
      "Epoch 90: 535.147168569\n",
      "Epoch 91: 531.536352663\n",
      "Epoch 92: 528.026869389\n",
      "Epoch 93: 524.616452118\n",
      "Epoch 94: 521.3029731\n",
      "Epoch 95: 518.084202511\n",
      "Epoch 96: 514.958069014\n",
      "Epoch 97: 511.922430302\n",
      "Epoch 98: 508.97507345\n",
      "Epoch 99: 506.113908571\n"
     ]
    }
   ],
   "source": [
    "\"\"\" EXAMPLE 1 \"\"\"\n",
    "\n",
    "# GOAL: Linear regression to find out if correlation exists between # fires & # thefts in Chicago\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import xlrd\n",
    "\n",
    "DATA_FILE = \"data/fire_theft.xls\"\n",
    "\n",
    "# Step 1: read in data from .xls file\n",
    "book = xlrd.open_workbook(DATA_FILE, encoding_override=\"utf-8\")\n",
    "sheet = book.sheet_by_index(0)\n",
    "data = np.asarray([sheet.row_values(i) for i in range(1, sheet.nrows)])\n",
    "n_samples = sheet.nrows - 1\n",
    "\n",
    "# Step 2: create placeholders for input X (# fires) and label Y (# thefts)\n",
    "X = tf.placeholder(tf.float32, name=\"X\")\n",
    "Y = tf.placeholder(tf.float32, name=\"Y\")\n",
    "\n",
    "# dummy data - does not work\n",
    "#X = np.linspace(-1.0, 1.0, 100.0)\n",
    "#Y = X * 3 + np.random.randn(X.shape[0]) * 0.5\n",
    "#X, Y = hash(tuple(X)), hash(tuple(Y))\n",
    "\n",
    "# Step 3: create weight and bias\n",
    "w = tf.Variable(0.01, name=\"weight\")\n",
    "u = tf.Variable(0.01, name=\"weights_2\")\n",
    "b = tf.Variable(0.0, name=\"bias\")\n",
    "\n",
    "# Step 4: build model to predict Y\n",
    "Y_predicted = X * w + b\n",
    "#Z_predicted = X * X * w + X * u + b                doesn't work?\n",
    "#print(Y_predicted.eval())\n",
    "#print(Z_predicted.eval())\n",
    "#Y_predicted += X * (w + w2)\n",
    "\n",
    "# Step 5: specify loss function\n",
    "loss = tf.square(Y - Y_predicted, name=\"loss\")\n",
    "\n",
    "# Step 6: create optimizer for loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Step 7: initialize the necessary variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    writer = tf.summary.FileWriter('./my_graph/03/linear_reg', sess.graph)\n",
    "    \n",
    "    # Step 8: train the model\n",
    "    for i in range(100):\n",
    "        total_loss = 0\n",
    "        for x, y in data:\n",
    "            # Session runs train_op and fetch values of loss\n",
    "            _, l = sess.run([optimizer, loss], feed_dict={X: x, Y: y})\n",
    "            total_loss += l\n",
    "        print('Epoch {0}: {1}'.format(i, total_loss/n_samples))\n",
    "        \n",
    "    writer.close()\n",
    "        \n",
    "    # Step 9: output the values of w and b\n",
    "    w_value, b_value = sess.run([w, b])\n",
    "    #w2_value = sess.run([w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe3e830ca50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the results\n",
    "X, Y = data.T[0], data.T[1]\n",
    "plt.plot(X, Y, 'bo', label='Real data')\n",
    "plt.plot(X, X * w_value + b_value, 'r', label='Predicted data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Huber Loss - to deal with Outliers (w/ large square loss) \"\"\"\n",
    "\n",
    "def huber_loss (labels, predictions, delta=1.0):\n",
    "    \"\"\" square if large, abs value if small\"\"\"\n",
    "    residual = tf.abs(predictions - labels)\n",
    "    condition = tf.less(residual, delta)\n",
    "    small_residual = 0.5 * tf.square(residual)\n",
    "    large_residual = delta * residual - 0.5 * tf.square(delta)\n",
    "    # if residual < delta, ret small_res; else, ret large_res\n",
    "    return tf.select(condition, small_res, large_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Linear Regression model '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Linear Regression model \"\"\"\n",
    "    \n",
    "#Y is one-hot\n",
    "# send X and Y in matches -- mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Average loss epoch 0: 0.690402577112\n",
      "Average loss epoch 1: 0.369173566327\n",
      "Average loss epoch 2: 0.324579209228\n",
      "Average loss epoch 3: 0.30455674106\n",
      "Average loss epoch 4: 0.291594126494\n",
      "Average loss epoch 5: 0.284206666973\n",
      "Average loss epoch 6: 0.276253950259\n",
      "Average loss epoch 7: 0.272331159692\n",
      "Average loss epoch 8: 0.268997845759\n",
      "Average loss epoch 9: 0.264387169831\n",
      "Total time: 5.76252698898 seconds\n",
      "Optimization Finished!\n",
      "Accuracy 0.9235\n"
     ]
    }
   ],
   "source": [
    "\"\"\" EXAMPLE 2 \"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Starter code for logistic regression model to solve OCR task \n",
    "with MNIST in TensorFlow\n",
    "MNIST dataset: yann.lecun.com/exdb/mnist/\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "\n",
    "# Define parameters for the model\n",
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "n_epochs = 10\n",
    "\n",
    "# Step 1: Read in data\n",
    "# using TF Learn's built in function to load MNIST data to the folder data/mnist\n",
    "mnist = input_data.read_data_sets('./data/mnist', one_hot=True) \n",
    "\n",
    "# Step 2: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name=\"image\")\n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10], name=\"label\")\n",
    "\n",
    "# Step 3: create weights and bias\n",
    "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
    "# b is initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w) + b\n",
    "# shape of b depends on Y\n",
    "w = tf.Variable(tf.random_normal(shape=[784, 10], stddev=0.01), name=\"weights\")\n",
    "b = tf.Variable(tf.zeros([1, 10]), name=\"bias\")\n",
    "\n",
    "# Step 4: build model\n",
    "# the model that returns the logits.\n",
    "# this logits will be later passed through softmax layer\n",
    "# to get the probability distribution of possible label of the image\n",
    "# DO NOT DO SOFTMAX HERE\n",
    "\n",
    "logits = tf.matmul(X, w) + b\n",
    "\n",
    "# Step 5: define loss function\n",
    "# use cross entropy loss of the real labels with the softmax of logits\n",
    "# use the method:\n",
    "# tf.nn.softmax_cross_entropy_with_logits(logits, Y)\n",
    "# then use tf.reduce_mean to get the mean loss of the batch\n",
    "\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y)\n",
    "loss = tf.reduce_mean(entropy)\n",
    "\n",
    "# Step 6: define training op\n",
    "# using gradient descent to minimize loss\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)              #92%\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)  #84%\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # to visualize with TensorBoard\n",
    "    writer = tf.summary.FileWriter('./my_graph/03/logistic_reg', sess.graph)\n",
    "                \n",
    "    start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\t\n",
    "    n_batches = int(mnist.train.num_examples/batch_size)\n",
    "    for i in range(n_epochs): # train the model n_epochs times\n",
    "        total_loss = 0\n",
    "\n",
    "        for _ in range(n_batches):\n",
    "            X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "            # run optimizer + fetch loss_batch\n",
    "            _, loss_batch = sess.run([optimizer, loss], feed_dict={X: X_batch, Y: Y_batch})\n",
    "            # \n",
    "            total_loss += loss_batch\n",
    "        print 'Average loss epoch {0}: {1}'.format(i, total_loss/n_batches)\n",
    "\n",
    "    print 'Total time: {0} seconds'.format(time.time() - start_time)\n",
    "\n",
    "    print('Optimization Finished!') # should be around 0.35 after 25 epochs\n",
    "\n",
    "    # test the model\n",
    "    n_batches = int(mnist.test.num_examples/batch_size)\n",
    "    total_correct_preds = 0\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "        logits_batch = sess.run(logits, feed_dict={X: X_batch, Y: Y_batch})\n",
    "        probs = tf.nn.softmax(logits_batch)\n",
    "        correct_probs = tf.equal(tf.argmax(probs, 1), tf.argmax(Y_batch, 1))\n",
    "        num_correct = tf.reduce_sum(tf.cast(correct_probs, tf.float32)) # need numpy.count_nonzero(boolarr) :(\n",
    "        total_correct_preds += sess.run(num_correct)\n",
    "\n",
    "print 'Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "\"\"\" MAXIMIZING MNIST PREDICTION \"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "\n",
    "# Define parameters for the model\n",
    "learning_rate = 0.01\n",
    "batch_size = None\n",
    "n_epochs = 20\n",
    "\n",
    "# Step 1: Read in data\n",
    "mnist = input_data.read_data_sets('./data/mnist', one_hot=True) \n",
    "\n",
    "# Step 2: create placeholders for features and labels\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name=\"image\")\n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10], name=\"label\")\n",
    "\n",
    "# Step 3: create weights and bias\n",
    "def weight_variable(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.01), name=\"weights\")\n",
    "def bias_variable(shape):\n",
    "    return tf.Variable(tf.constant(0.1, shape=shape), name=\"bias\")\n",
    "\n",
    "# Step 4: build model\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# first layer\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "# reshape to 4d tensor [batch, width, height, colorchannels]\n",
    "x_image = tf.reshape(X, [-1, 28, 28, 1])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# second layer\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# fully connected layer\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)    #turns off dropout during testing (of keep_prob = 1)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# readout layer\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "# Step 5: define loss function\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_conv, labels=Y))\n",
    "\n",
    "# Step 6: define training op\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(y_conv, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.09375\n",
      "step 1, training accuracy 0.953125\n",
      "step 2, training accuracy 0.9375\n",
      "step 3, training accuracy 0.9375\n",
      "step 4, training accuracy 0.96875\n",
      "step 5, training accuracy 1\n",
      "step 6, training accuracy 0.984375\n",
      "step 7, training accuracy 0.96875\n",
      "step 8, training accuracy 0.953125\n",
      "step 9, training accuracy 0.984375\n",
      "step 10, training accuracy 0.96875\n",
      "step 11, training accuracy 1\n",
      "step 12, training accuracy 0.96875\n",
      "step 13, training accuracy 0.984375\n",
      "step 14, training accuracy 1\n",
      "step 15, training accuracy 1\n",
      "step 16, training accuracy 0.984375\n",
      "step 17, training accuracy 1\n",
      "step 18, training accuracy 0.984375\n",
      "step 19, training accuracy 1\n",
      "step 20, training accuracy 0.984375\n",
      "step 21, training accuracy 1\n",
      "step 22, training accuracy 1\n",
      "step 23, training accuracy 0.984375\n",
      "step 24, training accuracy 0.96875\n",
      "Total time: 2241.22464395 seconds\n",
      "Optimization Finished!\n",
      "test accuracy 0.9912\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "if True:\n",
    "    writer = tf.summary.FileWriter('./my_graph/03/logistic_reg', sess.graph)\n",
    "                \n",
    "    start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\t\n",
    "    \n",
    "    batch_size = 64\n",
    "    #n_batches = int(mnist.train.num_examples/batch_size)\n",
    "    n_batches = 400\n",
    "    n_epochs = 25\n",
    "    for i in range(n_epochs): # train the model n_epochs times\n",
    "        if i % 1 == 0:\n",
    "            X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "            train_accuracy = accuracy.eval(feed_dict={X: X_batch, Y: Y_batch, keep_prob: 1.0})\n",
    "            print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "\n",
    "        for _ in range(n_batches):\n",
    "            X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "            # run optimizer + fetch loss_batch\n",
    "            optimizer.run(feed_dict={X: X_batch, Y: Y_batch, keep_prob: 0.5})\n",
    "        \n",
    "\n",
    "    print 'Total time: {0} seconds'.format(time.time() - start_time)\n",
    "\n",
    "    print('Optimization Finished!') # should be around 0.35 after 25 epochs\n",
    "\n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.9912\n"
     ]
    }
   ],
   "source": [
    "print('test accuracy %g' % accuracy.eval(feed_dict={X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
